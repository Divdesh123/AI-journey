{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c700e698",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the model learns patterns from unlabeled data. The goal is to discover the underlying structure, groupings, or features in the data without explicit supervision.\n",
    "\n",
    "## Key Concepts\n",
    "- **No labels**: Only input data (X), no output labels (y)\n",
    "- **Clustering**: Grouping similar data points (e.g., K-Means)\n",
    "- **Dimensionality Reduction**: Reducing the number of features (e.g., PCA)\n",
    "\n",
    "In this notebook, we'll explore unsupervised learning with Python examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794d356",
   "metadata": {},
   "source": [
    "## Clustering: K-Means\n",
    "\n",
    "K-Means is a popular clustering algorithm that partitions data into k clusters based on feature similarity.\n",
    "\n",
    "Let's see a K-Means clustering example using synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b12ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering Example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Fit K-Means\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Plot clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6b06b",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique to reduce the number of features in a dataset while retaining most of the variance. It is useful for visualization and speeding up learning algorithms.\n",
    "\n",
    "Let's see a PCA example using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Example on Iris Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot PCA result\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA on Iris Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa240de1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Unsupervised learning finds patterns in unlabeled data.\n",
    "- We explored clustering (K-Means) and dimensionality reduction (PCA) with Python examples.\n",
    "- These techniques are useful for data exploration, visualization, and preprocessing.\n",
    "\n",
    "Try experimenting with other algorithms like hierarchical clustering or t-SNE!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
